# 기본 학습 설정
training:
  # 기본 학습 설정
  num_train_epochs: 10
  learning_rate: 2e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  optim: "paged_adamw_8bit"
  
  # 스케줄러 설정 추가
  lr_scheduler_type: "cosine"  # 코사인 스케줄러 사용
  warmup_steps: 100           # 또는 warmup_ratio 대신 직접 스텝 수 지정
  
  # 배치 및 메모리 최적화
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16
  gradient_checkpointing: true
  
  # 하드웨어 최적화
  fp16: false
  bf16: true
  
  # 평가 및 저장 전략
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 1
  load_best_model_at_end: true
  metric_for_best_model: "eval/rouge1_f1"
  greater_is_better: true
  
  # 로깅 관련 설정
  logging_strategy: "steps"     # step 단위 로깅
  logging_steps: 10            # 10 스텝마다 로깅
  logging_first_step: true     # 첫 스텝부터 로깅
  logging_dir: "${general.output_path}/logs"
  report_to: ["wandb", "tensorboard"]  # wandb와 tensorboard 모두 사용
  
  # 생성 관련
  predict_with_generate: true
  remove_unused_columns: true
  generation_max_length: ${model.generation.max_new_tokens}
  generation_num_beams: ${model.generation.num_beams}

# 데이터 전처리 설정 추가
preprocessing:
  enabled: true
  merge_train_dev: true
  split_ratio: 0.1
  stratify_column: null
  stratify_by_length: true
  length_strategy: "ratio" #"both"
  length_bins:
    dialogue: 3  # 대화 텍스트 구간 수
    summary: 3   # 요약 텍스트 구간 수
    ratio: 3     # 비율 구간 수
  random_state: ${general.seed}
  balance_length: true
