name: "MLP-KTLim/llama-3-Korean-Bllossom-8B"
family: "llama"
mode: "finetune"

# 토크나이저 설정
tokenizer:
  max_length: 1024
  padding_side: "left"
  truncation_side: "left"

# 모델 설정
model:
  trust_remote_code: true
  torch_dtype: "float16"
  device_map: "auto"

# 생성 설정
generation:
  max_new_tokens: 128
  min_new_tokens: 10
  temperature: 0.3
  top_p: 0.95
  do_sample: true
  num_beams: 5
  length_penalty: 1.0
  repetition_penalty: 1.2
  no_repeat_ngram_size: 3

# 양자화 설정 강화
quantization:
  enabled: true
  precision: "int4"
  compute_dtype: "float16"
  double_quant: true
  quant_type: "nf4"

# LoRA 설정 최적화
lora:
  r: 4
  alpha: 16
  dropout: 0.1
  target_modules: [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
  ]
  task_type: "CAUSAL_LM"
  bias: "none"
  modules_to_save: null
  fan_in_fan_out: false
  inference_mode: false

# 학습 설정 최적화
train:
  training:
    # 기본 학습 설정
    num_train_epochs: 3
    learning_rate: 2e-4
    warmup_ratio: 0.1
    weight_decay: 0.01
    max_grad_norm: 1.0
    
    # 배치 및 메모리 최적화
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 2
    gradient_accumulation_steps: 8
    gradient_checkpointing: true
    
    # 하드웨어 최적화
    fp16: true
    bf16: false
    optim: "paged_adamw_8bit"
    
    # 평가 및 저장 전략
    evaluation_strategy: "epoch"
    save_strategy: "epoch"
    save_total_limit: 2
    load_best_model_at_end: true
    metric_for_best_model: "eval_rouge1_f1"
    greater_is_better: true
    
    # 로깅
    logging_steps: 10
    logging_first_step: true
    
    # 생성 관련
    predict_with_generate: true
    remove_unused_columns: true
 