name: "eenzeenee/t5-base-korean-summarization"
mode: "finetune"
family: "t5"

# 모델 특화 설정

partial_training: true
num_layers_to_train: 2
lora: false
quantization:
  enabled: false
  precision: "int8"
  compute_dtype: "float16"
tokenizer:
  max_length: 512 # 1024
  padding: "max_length"
  truncation: true
  return_tensors: "pt"
  encoder_max_len: 512 # 1024
  decoder_max_len: 200

generation:
  num_beams: 5
  # max_length: 150
  # min_length: 50
  length_penalty: 0.8
  repetition_penalty: 2.0
  no_repeat_ngram_size: 3
  early_stopping: true
  max_new_tokens: 200  # 200자 * 3토큰
  min_new_tokens: 50 # 100자 * 3토큰\
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true