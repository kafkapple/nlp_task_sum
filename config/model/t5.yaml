name: "eenzeenee/t5-base-korean-summarization"
mode: "finetune"
family: "t5"

tokenizer:
  max_length: 1024
  padding: "max_length"
  truncation: true
  return_tensors: "pt"
  encoder_max_len: 1024
  decoder_max_len: 128

generation:
  num_beams: 4
  max_length: 256
  min_length: 32
  temperature: 1.0
  top_p: 0.95
  repetition_penalty: 1.0
  length_penalty: 1.0
  early_stopping: true

train:
  training:
    num_train_epochs: 3
    learning_rate: 5e-5
    warmup_ratio: 0.1
    weight_decay: 0.01
    
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8
    gradient_accumulation_steps: 4
    gradient_checkpointing: true
    
    evaluation_strategy: "epoch"
    save_strategy: "epoch"
    save_total_limit: 2
    load_best_model_at_end: true
    metric_for_best_model: "eval_rouge1_f1"
    greater_is_better: true
    
    logging_steps: 10
    logging_first_step: true
    
    predict_with_generate: true
    remove_unused_columns: true 