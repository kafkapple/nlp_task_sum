# defaults:
#   - ../common  # common.yaml 참조

name: "eenzeenee/t5-base-korean-summarization"
mode: "finetune"
family: "t5"

# 모델 특화 설정
tie_word_embeddings: true  # 토큰 임베딩 공유 설정
tie_encoder_decoder: true  # 인코더-디코더 가중치 공유

partial_training: true
num_layers_to_train: 2
lora: false
quantization:
  enabled: false
  precision: "int8"
  compute_dtype: "float16"

# 모델 초기화 설정
init_weights: false #true
force_download: false
resume_from_checkpoint: false
tokenizer:
  max_length: 512
  max_target_length: 128  # 요약문 최대 길이
  padding: "max_length"
  truncation: true
  return_tensors: "pt"
  encoder_max_len: 512
  decoder_max_len: 200
  bos_token: "<s>"
  eos_token: "</s>"
  special_tokens:
    additional_special_tokens: ${common.additional_special_tokens}  # 참조 방식 수정

# 모델 기본 설정
model:
  name: "eenzeenee/t5-base-korean-summarization"
  family: "t5"

# 기본 생성 설정 (train.yaml이나 config.yaml에서 override 가능)
generation:
  max_new_tokens: 250  # max_length 대신 max_new_tokens 사용
  min_new_tokens: 50   # min_length 대신 min_new_tokens 사용
  num_beams: 5
  length_penalty: 0.6
  repetition_penalty: 2.0
  no_repeat_ngram_size: 3
  early_stopping: true
  bad_words_ids: null  # train.yaml이나 config.yaml에서 필요시 override

# Few-shot 설정
prompt:
  mode: "few-shot"  # "zero-shot" 또는 "few-shot"
  n_samples: 2      # 사용할 예제 수
  selection:
    method: "random"  # 예제 선택 방법
    seed: ${general.seed} 