# defaults:
#   - ../common  # 이 부분 제거

name: "gogamza/kobart-summarization"

mode: "finetune"
family: "bart"

# 모델 특화 설정만 포함
partial_training: true
num_layers_to_train: 2
lora: false
quantization:
  enabled: false
  precision: "int8"
  compute_dtype: "float16"

tokenizer:
  max_length: 512
  max_target_length: 128  # 요약문 최대 길이
  padding: "max_length"
  truncation: true
  return_tensors: "pt"
  encoder_max_len: 512
  decoder_max_len: 200
  bos_token: "<s>"
  eos_token: "</s>"
  special_tokens:
    additional_special_tokens: ${common.additional_special_tokens}  # 참조 방식 수정
# common 참조 방식 수정

generation:
  num_beams: 5
  max_new_tokens: 200  # 200자 * 3토큰
  min_new_tokens: 50   # 50자 * 3토큰
  length_penalty: 0.8
  repetition_penalty: 2.0
  no_repeat_ngram_size: 3
  early_stopping: true
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  length_ratio: 0.2  # 입력 길이 대비 생성 길이 비율
# Few-shot 설정
prompt:
  mode: "few-shot"  # "zero-shot" 또는 "few-shot"
  n_samples: 2      # 사용할 예제 수
  selection:
    method: "random"  # 예제 선택 방법
    seed: ${general.seed} 
# 또는
# mode: "finetune"
# family: "bart"
# tokenizer:
#   max_length: 1024
#   padding: "max_length"
#   truncation: true
#   return_tensors: "pt"
#   encoder_max_len: 1024
#   decoder_max_len: 128
#
# generation:
#   num_beams: 4
#   max_length: 256
#   min_length: 32
#   length_penalty: 1.0
#   early_stopping: true
#
# train:
#   training:
#     num_train_epochs: 3
#     learning_rate: 5e-5
#     warmup_ratio: 0.1
#     weight_decay: 0.01
#     
#     per_device_train_batch_size: 8
#     per_device_eval_batch_size: 8
#     gradient_accumulation_steps: 4
#     gradient_checkpointing: true
#     
#     evaluation_strategy: "epoch"
#     save_strategy: "epoch"
#     save_total_limit: 2
#     load_best_model_at_end: true
#     metric_for_best_model: "eval_rouge1_f1"
#     greater_is_better: true
#     
#     logging_steps: 10
#     logging_first_step: true
#     
#     predict_with_generate: true
#     remove_unused_columns: true

