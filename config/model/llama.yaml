name: "meta-llama/Llama-2-7b-chat-hf"
type: "llama"
mode: "prompt"

# 프롬프트 템플릿 설정
prompt_templates:
  v1:
    few_shot: |
      Below is a dialogue and its summary. Learn from this example:

      Dialogue: {sample_dialogue}
      Summary: {sample_summary}

      Now summarize the following dialogue:
      Dialogue: {dialogue}
      Summary:
    zero_shot: |
      Summarize the following dialogue:
      Dialogue: {dialogue}
      Summary:
  v2:
    few_shot: |
      Here's how to summarize a dialogue:
      
      Example Dialogue: {sample_dialogue}
      Example Summary: {sample_summary}
      
      Now it's your turn:
      Dialogue: {dialogue}
      Summary:
    zero_shot: |
      Create a concise summary of this dialogue:
      Dialogue: {dialogue}
      Summary:

# 토크나이저 설정
tokenizer:
  max_length: 2048
  padding_side: "left"
  truncation_side: "left"

# 모델 설정
model:
  trust_remote_code: true
  torch_dtype: "float16"
  device_map: "auto"
  # 생성 설정
  generation:
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.9
    repetition_penalty: 1.1

# 양자화 설정
quantization:
  enabled: true
  precision: "int4"
  compute_dtype: "float16"

# LoRA 설정
lora:
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "v_proj"] 